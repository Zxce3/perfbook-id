% intro/intro.tex
% mainfile: ../perfbook.tex
% SPDX-License-Identifier: CC-BY-SA-3.0

\QuickQuizChapter{chp:Introduction}{Introduction}{qqzintro}
%
\Epigraph{Jika pemrograman paralel itu sulit, mengapa ada begitu banyak program
	  paralel?}{Tidak diketahui}

Pemrograman paralel telah mendapatkan reputasi sebagai salah satu area tersulit
yang bisa ditangain oleh para hacker. Makalah dan buku-buku memberi peringatan
tentang bahaya \IX{deadlock}, \IX{livelock}, \IXpl{race condition},
non-determinisme, batasan \IXaltr{Amdahl's-Law}{Amdahl's Law} untuk skala, dan
latensi waktu nyata yang berlebihan. Dan bahaya-bahaya ini benar-benar nyata;
kami penulis telah mengumpulkan
% 2020: 30 for Paul E. McKenney
pengalaman yang tidak terhitung jumlahnya selama bertahun-tahun, bersama dengan
luka-luka emosional dan rambut rontok.

Namun, teknologi baru yang sulit digunakan saat pengenalan selalu cakap menjadi
lebih mudah dari waktu ke waktu. Sebagai contoh, kemampuan untuk mengendarai
mobil yang dulunya langka kini menjadi lumrah di banyak negara. Perubahan
dramatis ini terjadi karena dua alasan dasar:
\begin{enumerate*}[(1)]
\item Mobil menjadi lebih murah dan mudah didapatkan, sehingga lebih banyak
orang memiliki kesempatan untuk belajar mengendarai, dan
\item Mobil menjadi lebih mudah untuk dioperasikan karena transmisi otomatis,
pengisian bahan bakar otomatis, pengapian otomatis, keandalan yang sangat
ditingkatkan, dan sejumlah perbaikan teknologi lainnya.
\end{enumerate*}

Hal yang sama berlaku untuk banyak teknologi lain, termasuk komputer. Tidak lagi
diperlukan mesin ketik untuk mengoperasikan komputer. Spreadsheet memungkinkan
sebagian besar orang yang tidak berpengalaman untuk mendapatkan hasil dari
komputer yang membutuhkan tim spesialis beberapa dekade lalu. Contoh yang paling
menarik adalah web-surfing dan pembuatan konten, yang sejak awal 2000-an dapat
dilakukan dengan mudah oleh orang-orang yang tidak terlatih, tidak berpendidikan
menggunakan berbagai peralatan jejaring sosial yang sekarang sudah umum.
Baru-baru ini, pada tahun 1968, pembuatan konten semacam itu merupakan
proyek~\cite{DouglasEngelbart1968} penelitian yang jauh, yang pada saat itu
dijelaskan sebagai ``seperti pendaratan UFO di halaman gedung
putih''~\cite{ScottGriffen2000}.
% http://www.ibiblio.org/pioneers/engelbart.html
% http://www.histech.rwth-aachen.de/www/quellen/engelbart/ahi62index.html

Oleh karena itu, jika Kamu ingin berpendapat bahwa pemrograman paralel akan
tetap sesulit yang dirasakan oleh banyak orang saat ini, kamu yang akan
menanggung beban pembuktian, harus diingat bahwa contoh tandingnya selama
berabad-abad di banyak di bidang usaha.

\section{Historic Parallel Programming Difficulties}
\label{sec:intro:Historic Parallel Programming Difficulties}
%
\epigraph{Bukan kekuatan untuk mengingat, tetapi kebalikannya, kekuatan untuk
	  melupakan, yang merupakan syarat utama keberadaan kita.} {Sholem Asch}

Sesuai dengan judulnya, buku ini mengambil pendekatan yang berbeda. Alih-alih
mengeluh tentang kesulitan pemrograman paralel, ia malah meneliti alasan mengapa
pemrograman paralel sulit, dan kemudian bekerja membantu pembaca mengatasi
kesulitan tersebut. Seperti yang akan terlihat, kesulitan historis ini terbagi
ke dalam beberapa kategori, termasuk:

\begin{enumerate}
\item	Harga tinggi dan kejaran yang relatif langka sistem paralel.
\item 	Kurangnya pengalaman peneliti dan praktisi dengan sistem pararel
\item 	Kurangnya kode paralel yang dapat diakses publik.
\item 	Kurangnya disiplin teknik pemrograman paralel yang dipahami secara luas.
\item 	{\it overhead} komunikasi yang tinggi relatif terhadap pemrosesan,
	bahkan di komputer shared-memori yang dipasangkan dengan erat.
\end{enumerate}

Banyak dari sejarah kesulitan ini sedang berjalan menuju penyelesaian. Pertama,
selama beberapa dekade terakhir, biaya sistem paralel telah menurun dari banyak
kali lipat dari biaya sebuah rumah ke biaya sebuah makanan yang murah, karena
\IXr{Moore's Law}~\cite{GordonMoore1965MooresLaw}.
Makalah yang memanggil keuntungan sistem CPU multi-core telah diterbitkan sejak
1996~\cite{Olukotun96}. IBM memperkenalkan multi-threading simultan ke dalam
keluarga \Power{} mereka pada tahun 2000, dan multi-core pada tahun 2001. Intel
memperkenalkan hyperthreading ke dalam barisan komoditas Pentium pada November
2000, dan baik AMD maupun Intel memperkenalkan CPU dual-core pada tahun 2005.
Sun mengikuti dengan multi-core/multi-threaded Niagara pada akhir 2005.
Faktanya, pada tahun 2008, mulai sulit untuk menemukan desktop dengan satu CPU,
dengan CPU single-core ditempatkan pada netbook dan perangkat terintegrasi. Pada
tahun 2012, bahkan smartphone mulai dilengkapi dengan multi-core CPU. Pada tahun
2020, standar krisis keamanaan mulai menangani konkuren.

Kedua, kemunculan sistem multi-core dengan biaya yang murah dan mudah diperoleh
berarti bahwa pengalaman yang dulunya langka dalam pemrograman paralel kini
tersedia untuk hampir semua peneliti dan praktisi. Faktanya, sistem paralel
telah lama berada dalam anggaran mahasiswa penghobi. Oleh karena itu, kami
mengharapkan tingkat penemuan dan inovasi yang tinggi disekitar sistem paralel.
dan meningkatkan keakraban dari waktu ke waktu akan membuat bidang pemrograman
paralel yang dulunya mahal menjadi jauh lebih ramah dan biasa.

Ketiga, pada abad ke-20, sistem paralel yang besar dan sangat paralel hampir
selalu dijaga ketat rahasia kepemilikannya. Sebaliknya, abad ke-21 telah melihat
banyak proyek perangkat lunak paralel open-source (dan demikian dapat diakses
publik), termasuk kernel Linux~\cite{Torvalds2.6kernel}, sistem
database~\cite{PostgreSQL2008,MySQL2008}, dan sistem
pesan~\cite{OpenMPI2008,BOINC2008}. Buku ini akan mengambil kernel Linux sebagai
contoh utama, tetapi akan memberikan banyak materi yang cocok untuk aplikasi
user-level.

Keempat, meskipun proyek-proyek pemrograman paralel skala besar pada 1980-an dan
1990-an hampir selalu merupakan proyek berpemilik. proyek-proyek ini telah
menanamkan komunitas lain dengan kader pengembang yang memahami disiplin teknik
yang diperlukan untuk mengembangkan kode paralel berkualitas produksi.

Sayangnya, kesulitan kelima, biaya komunikasi yang relatif tinggi terhadap biaya
pemrosesan, masih berlaku. Kesulitan ini telah menerima perhatian yang semakin
meningkat selama abad ke-21. Namun, menurut \ppl{Stephen}{Hawking}, kecepatan
cahaya yang terbatas dan bentuk atomik dari materi akan membatasi kemajuan di
bidang ini~\cite{BryanGardiner2007,GordonMoore03a}. Sayangnya, kesulitan ini
telah berlaku sejak akhir 1980-an, sehingga disiplin ilmu teknik yang telah
dikembangkan untuk mengatasi kesulitan ini telah berkembang menjadi praktis dan
efektif. Selain itu, desainer perangkat keras semakin sadar akan masalah ini,
Jadi mungkin perangkat keras di masa depan akan lebih ramah terhadap perangkat
lunak paralel, seperti yang dibahas dalam \cref{sec:cpu:Hardware Free Lunch?}.

\QuickQuiz{
	Ayolah!!!
	Pemrograman paralel telah dikenal sebagai sesuatu yang sangat sulit sejak
	beberapa dekade. Kamu tampaknya mengindikasikan bahwa itu tidak begitu
	sulit. Apa jenis permainan yang kamu mainkan?
}\QuickQuizAnswer{
	Jika kamu benar-benar percaya bahwa pemrograman paralel sangat sulit, maka
	kamu harus memiliki jawaban yang siap untuk pertanyaan ``Kenapa pemrograman
	paralel sulit?'' Seseorang dapat mencantumkan sejumlah alasan, mulai dari
	jalan buntu hingga konflik akses, tetapi jawaban yang sebenarnya adalah
	bahwa {\em itu tidak begitu sulit}. Sebagai contoh, jika pemrograman paralel
	benar-benar begitu sulit, mengapa banyak proyek open-source, mulai dari
	Apache hingga MySQL hingga kernel Linux, dapat menguasainya?

	Pertanyaan yang lebih baik mungkin bisa jadi: ``Kenapa pemrograman paralel
	\emph{dipersepsikan} sebagai sesuatu yang sulit?''
	Untuk melihat jawabannya, mari kita kembali ke tahun 1991. Paul McKenney
	sedang berjalan-jalan di parkiran sebuah toko menuju ke Pusat benchmarking
	membawa enam board CPU dual-80486 Sequent Symmetry. dan dia mulai menyadari
	bahwa dia membawa beberapa kali lipat harga rumah yang baru saja ia beli.
	\footnote{Ya, kesadaran yang datang secara tiba-tiba ini membuat dia
		berjalan lebih hati-hati.
		Mengapa kamu bertanya?}
	Biaya sistem paralel yang tinggi
	Pemrograman paralel dibatasi untuk beberapa orang yang istimewa
	bekerja untuk perusahaan yang memproduksi atau mampu
	Membeli mesih seharga lebih dari \$100.000---pada tahun 1991 dolar AS.

	Secara kontras, pada tahun 2020, Paul mengetik kata-kata ini pada laptop
	x86 berjaringan 6-core. Tidak seperti board CPU dual-80486, laptop ini
	juga berisi 64\,GB memori utama, disk solid-state 1\,TB, tampilan,
	Ethernet, port USB, wireless, dan Bluetooth. Dan laptop ini jauh lebih murah
	daripada satu board CPU dual-80486, bahkan sebelum memperhitungkan inflasi.

	Sistem paralel telah benar-benar tiba.
	Mereka tidak lagi menjadi domain yang dikuasai oleh sekelompok orang
	istimewa, tetapi sesuatu yang tersedia untuk hampir semua orang.

	Ketersediaan yang terbatas dari perangkat keras paralel adalah
	\emph{alasan sebenarnya} bahwa pemrograman paralel dianggap sulit.
	Begitu sulit untuk belajar untuk memprogram mesin yang paling sederhana
	jika kamu tidak memiliki akses ke mesin itu.
	Semenjak usia mesin paralel yang langka dan mahal telah menjadi masa lalu,
	usia ketika pemrograman paralel dianggap sangat sulit akan segera berakhir.
	\footnote{
		Pemrograman paralel dalam beberapa hal lebih sulit daripada
		pemrograman berurutan, misalnya, validasi paralel lebih sulit.
		Tetapi tidak lagi sangat sulit.}
}\QuickQuizEnd

Tetapi, mesikipun pemrograman paralel tidak sepeti yang dipersepsikan, itu
tetap membutuhkan banyak kerja. Pemrograman paralel tidak lebih sulit daripada
pemrograman berurutan.

\QuickQuiz{
	Bagaimana permrograman paralel \emph{bisa} lebih mudah daripada pemrograman
	berurutan?
}\QuickQuizAnswer{
	Ini bergantung pada lingkungan pemrograman. SQL~\cite{DIS9075SQL92} adalah
	sukses yang kurang dihargai, sebagai contoh, memungkinkan programmer yang
	tidak tahu apa-apa tentang paralelisme untuk menjaga sistem paralel yang
	besar tetap produktif. Kami dapat mengharapkan variasi lebih lanjut pada
	tema ini selama komputer paralel terus menjadi lebih murah dan mudah
	didapatkan. Sebagai contoh, salah satu pesaing yang mungkin di bidang ilmu
	dan teknik ilmiah adalah MATLAB*P, yang merupakan upaya untuk secara
	otomatis memparalelkan operasi matriks umum.

	Akhirnya, di Linux dan UNIX, pertimbangkan perintah shell berikut:

	\begin{VerbatimU}
	get_input | grep "interesting" | sort
	\end{VerbatimU}

	Pipeline ini memproses data yang masuk dari \co{get_input} dan mengirimkan
	hasilnya ke \co{grep} dan \co{sort} di proses didalam paralel.
	Bagaimana, itu tidak terlalu sulit, bukan?

	Secara singkat, pemrograman paralel sama mudahnya dengan pemrograman
	berurutan---setidaknya dalam lingkungan yang menyembunyikan paralelisme
	dari pengguna!
}\QuickQuizEnd

Oleh karena itu, itu masuk akal untuk mempertimbangkan alternatif dari
pemrograman paralel. Namun, tidak mungkin untuk mempertimbangkan alternatif
pemrograman paralel tanpa memahami tujuan pemrograman paralel. Topik ini dibahas
pada bagian berikut.

\section{Parallel Programming Goals}
\label{sec:intro:Parallel Programming Goals}
%
\epigraph{If you don't know where you are going, you will end up somewhere
	  else.}{Yogi Berra}

Tiga tujuan utama pemrograman paralel (di atas dan di luar tujuan
pemrograman berurutan) adalah sebagai berikut:

\begin{enumerate}
\item	\IX{Kinerja}.
\item	\IX{Produktivitas}.
\item	\IX{Umum}.
\end{enumerate}

Sayangnya, mengingat keadaan seni saat ini, mungkin hanya dua dari tiga tujuan
ini yang dapat dicapai untuk setiap program paralel. Tujuan-tujuan ini
maka menjadi segitiga \emph{besi} dari pemrograman paralel, segitiga yang
pada akhirnya akan menyebabkan harapan yang terlalu optimis hancur.\footnote{
	Kudos untuk Michael Wong yang telah memberi nama segitiga besi.}

\QuickQuizSeries{%
\QuickQuizB{
	Oh, yang benar???
	Bagaimana dengan kebenaran, keterjangkauan, ketahanan, dan lain-lain?
}\QuickQuizAnswerB{
	Ini adalah tujuan penting, tetapi sama pentingnya untuk program
	berurutan. Oleh karena itu, penting meskipun mereka, mereka tidak
	terdapat pada daftar khusus pemrograman paralel.
}\QuickQuizEndB
%
\QuickQuizM{
	Dan jika kebenaran, keterjangkauan, dan ketahanan tidak ada di daftar,
	kenapa produktivitas dan umum?
}\QuickQuizAnswerM{
	Mengingat bahwa pemrograman paralel dianggap lebih sulit dari pemrograman
	berurutan, produktivitas sama dan Oleh karena itu tidak boleh dihilangkan.
	Selanjutnya, jika lingkungan pemrograman paralel dengan produktivitas yang
	tinggi seperti SQL melayani tujuan tertentu, maka umumnya harus juga
	ditambahkan ke daftar.
}\QuickQuizEndM
%
\QuickQuizM{
	Mengingat bahwa program paralel jauh lebih sulit untuk terbukti benar
	daripada program berurutan, sekali lagi, bukankah kebenaran \emph{benar
	-benar} ada dalam daftar?
}\QuickQuizAnswerM{
	Dari sudut pandang teknik, kesulitan dalam membuktikan kebenaran, baik
	secara formal maupun informal, akan menjadi penting sejauh itu berdampak
	pada tujuan utama produktivitas.Jadi, dalam kasus di mana bukti kebenaran
	itu penting, mereka dimasukkan ke dalam rubrik `` produktivitas ''.
}\QuickQuizEndM
%
\QuickQuizE{
	Bagaimana dengan bersenang-senang saja?
}\QuickQuizAnswerE{
	Bersenanng-senang juga penting, tetapi, kecuali kamu adalah tidak hobbyist ,
	biasanya tidak akan menjadi tujuan \emph{utama}. Sementara itu, jika kamu
	\emph{adalah} hobbyist, silakan bermain!
}\QuickQuizEndE
}

Setiap tujuan ini dijelaskan lebih lanjut pada bagian berikut.

\subsection{Performance}
\label{sec:intro:Performance}

Performa adalah tujuan utama dari sebagian besar usaha pemrograman paralel.
Setelah semua, jika kinerja bukanlah masalah, mengapa tidak melakukan dirimu
sendiri suatu kebaikan: Hanya menulis kode berurutan, dan bahagia? Itu akan
sangat jauh lebih mudah.

\QuickQuiz{
	Apakah tidak ada kasus di mana pemrograman paralel adalah tentang sesuatu
	selain kinerja?
}\QuickQuizAnswer{
	Tentu saja ada kasus dimana masalah yang akan diselesaikan adalah paralel
	secara alami, misalnya, metode {\it Monte Carlo} dan beberapa komputasi
	numerik. Bahkan dalam kasus-kasus ini, bagaimanapun, akan ada sejumlah kerja
	tambahan dalam mengelola paralelisme.

	Paralelisme juga sering digunakan untuk keandalan. Untuk satu contoh,
	redundansi triple-modulo memiliki tiga sistem yang akan berjalan dalam
	paralel dan memilih hasilnya. Dalam kasus ekstrem, tiga sistem akan
	diimplementasikan secara independen menggunakan algoritma dan teknologi yang
	berbeda.
}\QuickQuizEnd

Harap dicatat bahwa ``kinerja'' diinterpretasikan secara luas di sini,
termasuk, misalnya, \IX{skalabilitas} (kinerja per CPU) dan \IX{efisiensi}
(kinerja per watt).

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/clockfreq}}
\caption{MIPS/Clock-Frequency Trend for Intel CPUs}
\label{fig:intro:Clock-Frequency Trend for Intel CPUs}
\end{figure}

Itu menyebutkan bahwa, fokus kinerja telah bergeser dari perangkat keras ke
perangkat lunak paralel.
Perubahan fokus ini disebabkan oleh fakta bahwa, meskipun \IXr{Moore's Law}
terus menghasilkan peningkatan dalam densitas transistor, ia telah berhenti
memberikan kenaikan kinerja berurutan.
Hal ini dapat dilihat di
\cref{fig:intro:Clock-Frequency Trend for Intel CPUs}.\footnote{
	plot ini menunjukkan frekuensi jam untuk CPU yang lebih baru secara
	teoretis mampu menghentikan satu atau lebih instruksi per jam, dan MIPS
	(jutaan instruksi per detik, biasanya dari Dhrystone lama)
	untuk CPU yang lebih lama yang memerlukan beberapa jam untuk mengeksekusi
	bahkan instruksi paling sederhana.
	Alasan untuk bergeser antara dua ukuran ini adalah bahwa kemampuan
	CPU yang lebih baru untuk menghentikan beberapa instruksi per jam
	umumnya terbatas oleh kinerja sistem memori.
	Selanjutnya, benchmark yang umum digunakan pada CPU yang lebih lama
	tertua, dan sulit untuk menjalankan benchmark yang lebih baru
	pada sistem yang berisi CPU lama, sebagian karena
	sulit untuk menemukan contoh yang berfungsi dari CPU lama.
}
yang menunjukkan bahwa menulis kode satu atau dua tahun tunggu untuk CPU untuk mengejar ketinggalan mungkin tidak lagi menjadi pilihan. Menghidupkan tren baru-baru ini pada bagian dari semua produsen utama menuju sistem Multicore/Multithreaded, paralelisme adalah cara untuk majuingin memanfaatkan kinerja penuh dari sistem mereka.

\QuickQuiz{
	Mengapa tidak sekalian menulis ulang program dari bahasa skrip yang
	inefisien ke C atau C++?
}\QuickQuizAnswer{
	Jika developer, anggaran, dan waktu tersedia untuk menulis ulang
	program seperti itu, dan jika hasilnya akan mencapai tingkat
	kinerja yang diperlukan pada satu CPU, ini bisa menjadi pendekatan
	yang masuk akal.
}\QuickQuizEnd

Jika begitu, tujuan awal adalah kinerja, bukan skala.
Terlepas dari itu, cara termudah untuk mencapai skala linier adalah
mengurangi kinerja setiap CPU~\cite{LinusTorvalds2001a}.
Dengan sistem empat-CPU, mana yang kamu pilih?
Program yang memberikan 100 transaksi per detik pada satu CPU,
tetapi tidak skala sama sekali?
Atau program yang memberikan 10 transaksi per detik pada satu CPU,
tetapi skala sempurna?
Program pertama tampaknya lebih baik, meskipun jawaban mungkin berubah
jika kamu memiliki sistem 32-CPU.

Yang mengatakan, hanya karena kamu memiliki beberapa CPU bukan berarti
itu adalah alasan untuk menggunakan semua, terutama karena penurunan
harga sistem multi-CPU terbaru.
Poin kunci untuk memahami adalah bahwa pemrograman paralel adalah
optimasi kinerja, dan, sebagai akibatnya, adalah salah satu optimasi
potensial dari banyak.
Jika programmu cukup cepat seperti yang ditulis sekarang, tidak ada
alasan untuk mengoptimalkan, baik dengan paralelisasi atau dengan
memberinya beberapa optimasi berurutan lainnya.\footnote{
	Jika kamu adalah hobi yang berfokus pada menulis perangkat lunak
	paralel, itu sudah cukup alasan untuk paralelisasi apa pun yang
	kamu minati.}
Dengan kata yang sama, jika kamu mencari untuk menerapkan paralelisme
sebagai optimasi untuk program berurutan, kamu akan perlu membandingkan
algoritma paralel dengan algoritma terbaik berurutan.
Ini mungkin memerlukan sedikit perhatian, karena terlalu banyak publikasi
mengabaikan kasus berurutan ketika menganalisis kinerja algoritma paralel.

\subsection{Productivity}
\label{sec:intro:Productivity}

\EQuickQuiz{
	Kenapa semua ini berbicara tentang masalah non-teknis??
	Dan bukan hanya \emph{sembarang} masalah non-teknis, tetapi
	\emph{produktivitas} dari semua hal?
	Mengapa harus peduli?
}\EQuickQuizAnswer{
	Jika kamu hanya sekedar hobi, mungkin kamu tidak perlu peduli.
	Tetapi bahkan hobi yang hanya sekedar akan sering peduli tentang
	seberapa banyak yang bisa mereka lakukan, dan seberapa cepat.
	Setelah semua, alat hobi yang paling populer biasanya adalah
	yang paling cocok untuk pekerjaan, dan bagian penting dari
	definisi ``paling cocok'' melibatkan produktivitas.
	Dan jika seseorang membayar kamu untuk menulis kode paralel,
	mereka akan sangat peduli dengan produktivitasmu.
	Dan jika orang yang membayarmu peduli tentang sesuatu, kamu akan
	lebih bijak untuk memperhatikan setidaknya beberapa hal itu!

	Di sisi lain, jika kamu \emph{memang} tidak peduli tentang produktivitas,
	mungkin kamu tidak perlu mempelajari paralelisme.
}\EQuickQuizEnd

\IX{Produktivitas} telah menjadi semakin penting dalam beberapa dekade terakhir.
Untuk melihat ini, pertimbangkan bahwa harga komputer awal adalah
puluhan juta dolar pada saat gaji insinyur hanya beberapa ribu dolar per tahun.
Jika menugaskan tim sepuluh insinyur untuk mesin seperti itu akan meningkatkan
kinerjanya, bahkan sebesar 10\,\%, maka gaji mereka
akan dibayar puluhan kali lipat.

Satu dari beberapa mesin adalah CSIRAC, komputer program yang masih
tertahan, yang ditempatkan dalam operasi pada tahun 1949~\cite{CSIRACMuseumVictoria,CSIRACUniversityMelbourne}.
Karena mesin ini dibangun sebelum era transistor, mesin ini dibangun
dari 2.000 tabung vakum, berjalan dengan frekuensi 1\,kHz perjam,
mengkonsumsi 30\,kW daya, dan berat lebih dari tiga ton metrik.
Diketahui bahwa mesin ini memiliki 768 kata RAM, maka dapat disimpulkan
bahwa mesin ini tidak menderita dari masalah produktivitas yang sering
menyerang proyek perangkat lunak skala besar hari ini.

Hari ini, mesin seperti itu akan sangat sulit dibeli dengan
sangat sedikit daya komputasi.
Mungkin contoh sederhanya seperti 8-bit embedded microprocessor yang
disebutkan di Z80~\cite{z80Wikipedia}, tetapi bahkan Z80 lama
memiliki frekuensi CPU lebih dari 1.000 kali lebih cepat dari CSIRAC.
Z80 CPU memiliki 8.500 transistor, dan dapat dibeli pada tahun 2008
dengan harga kurang dari \$2 US per unit dalam jumlah 1.000 unit.
Dalam kontras dengan CSIRAC, biaya pengembangan perangkat lunak
sangat tidak signifikan untuk Z80.

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{SMPdesign/mipsperbuck}}
\caption{MIPS per Die for Intel CPUs}
\label{fig:intro:MIPS per Die for Intel CPUs}
\end{figure}

CSIRAC dan Z80 adalah dua titik dalam tren jangka panjang, seperti
dapat dilihat di \cref{fig:intro:MIPS per Die for Intel CPUs}.
Gambar ini menampilkan perkiraan daya komputasi per die selama
empat dekade terakhir, menunjukkan peningkatan enam orde besar
selama periode empat puluh tahun.
Perhatikan bahwa munculnya CPU multicore telah memungkinkan peningkatan
ini terus berlanjut meskipun tembok frekuensi jam tangan terjadi pada
tahun 2003, meskipun dengan biaya dies yang mendukung lebih dari 50
thread perangkat keras masing-masing.

Salah satu konsekuensi yang tak terhindarkan dari penurunan drastis dari biaya
perangkat keras adalah bahwa produktivitas perangkat lunak menjadi semakin
penting.
Tidak lagi cukup hanya untuk memanfaatkan perangkat keras secara
efisien: sekarang perlu untuk menggunakan pengembang perangkat lunak yang lebih
efisien juga.Ini telah lama menjadi kasus untuk perangkat keras berurutan,
tetapi perangkat keras paralel telah menjadi komoditas berbiaya rendah baru-baru
ini.Oleh karena itu, hanya baru -baru ini produktivitas tinggi menjadi lebih
penting saat membuat perangkat lunak paralel.

\QuickQuiz{
	Dengan murahnya sistem paralel, bagaimana seseorang bisa
	membayar orang untuk mengembangkan perangkat lunak untuk mereka?
}\QuickQuizAnswer{
	Ada beberapa jawaban untuk pertanyaan ini:
	\begin{enumerate}
	\item 	Diberikan sebuah klaster komputasi besar dari mesin paralel,
		biaya agregat klaster bisa dengan mudah mempertimbangkan
		usaha pengembangan yang besar, karena biaya pengembangan
		dapat dibagi dengan banyaknya mesin.
	\item	Perangkat lunak populer yang dijalankan oleh puluhan juta pengguna
		dapat dengan mudah membenarkan upaya pengembang yang substansial,
		karena biaya pengembangan ini dapat disebarkan di atas puluhan
		jutaan pengguna.
		Perhatikan bahwa ini termasuk hal -hal seperti kernel dan sistem
		Perpustakaan.
	\item 	Jika mesin paralel murah digunakan untuk mengontrol operasi
		suatu peralatan yang bernilai tinggi, maka biaya peralatan
		ini mungkin dengan mudah mempertimbangkan usaha pengembangan
		yang besar.
	\item	Jika perangkat lunak untuk mesin paralel murah menghasilkan
		hasil yang sangat bernilai (misalnya, penghematan energi),
		maka hasil yang bernilai ini mungkin dengan mudah mempertimbangkan
		usaha pengembangan yang besar.
	\item	Sistem kritis keamanan melindungi nyawa, yang dapat dengan
		jelas mempertimbangkan usaha pengembangan yang sangat besar.
	\item	Hobi dan peneliti mungkin mencari pengetahuan, pengalaman,
		hiburan, atau kehormatan.
	\end{enumerate}
	Jadi ini bukanlah kasus bahwa biaya perangkat keras yang murah
	membuat perangkat lunak tidak berharga, tetapi lebih bahwa
	tidak lagi mungkin untuk ``menyembunyikan'' biaya pengembangan
	perangkat lunak dalam biaya perangkat keras, setidaknya tidak
	kecuali ada jumlah besar perangkat keras.
}\QuickQuizEnd

Mungkin pada satu waktu, satu-satunya tujuan perangkat lunak paralel adalah
kinerja. Namun, sekarang, produktivitas juga mendapatkan sorotan.

\subsection{Generality}
\label{sec:intro:Generality}

Salah satu cara untuk menjamin biaya tinggi pengembangan perangkat lunak paralel
adalah untuk berusaha untuk memaksimalkan \IX{generality}. Semua hal lain yang
sama, biaya perangkat lunak yang lebih umum dapat dibagi dengan lebih banyak
pengguna daripada perangkat lunak yang kurang umum. Faktanya, gaya ekonomi ini
menjelaskan banyak fokus manik-manik pada portabilitas, yang dapat dilihat
sebagai kasus khusus penting dari generality.\footnote{
	Apresiasi untuk Michael Wong yang telah menunjukkan hal ini.}

Sayangnya, umumnya datang dengan biaya kinerja, produktivitas, atau keduanya.
Misalnya, portabilitas sering dicapai melalui lapisan adaptasi, yang tentu saja
mengakibatkan biaya kinerja. Untuk melihat hal ini secara lebih umum,
pertimbangkan lingkungan pemrograman paralel berikut:

\begin{description}
\item[C/C++ ``Locking'':] Kategori ini, yang mencakup POSIX Threads
	(pthreads)~\cite{OpenGroup1997pthreads}, Windows Threads, dan
	banyak lingkungan kernel sistem operasi, menawarkan kinerja yang
	luar biasa (setidaknya dalam batas-batas sistem SMP tunggal) dan
	juga menawarkan keumuman yang baik.
	Maafkan produktivitas yang relatif rendah.
\item[Java:] Lingkungan pemrograman umum dan secara alami multithreaded
	ini sangat dipercaya menawarkan produktivitas yang jauh lebih tinggi
	dari C atau C++, karena adanya kolektor sampah otomatis dan kumpulan
	kelas pustaka yang kaya.
	Tetapi, kinerjanya, meskipun telah ditingkatkan secara signifikan
	pada awal 2000an, tertinggal dari C dan C++.
\item[MPI:] Antarmuka Pesan Passing~\cite{MPIForum2008} ini menggerakkan
	klaster komputasi ilmiah dan teknis terbesar di dunia dan menawarkan
	kinerja dan skala yang tak tertandingi.
	Dalam teori, itu umum, tetapi itu sebagian besar digunakan untuk
	komputasi ilmiah dan teknis.
	Produktivitasnya diyakini oleh banyak orang untuk sangat rendah
	dibandingkan dengan lingkungan C/C++ ``locking plus threads''.
\item[OpenMP:] Kumpulan direktif kompiler ini dapat digunakan untuk
	paralelisasi loop.
	Ia sangat spesifik untuk tugas ini, dan spesifikitas ini seringkali
	mengurangi kinerjanya.
	Tetapi, ia jauh lebih mudah digunakan daripada MPI atau C/C++
	``locking plus threads.''
\item[SQL:] Structured Query Language~\cite{DIS9075SQL92} spesifik untuk
	perintah database relasional.
	Tetapi, kinerjanya cukup baik seperti yang diukur oleh hasil benchmark
	Transaction Processing Performance Council (TPC)~\cite{TPC}.
	Produktivitasnya sangat baik; sebenarnya, lingkungan pemrograman
	paralel ini memungkinkan orang untuk membuat penggunaan yang baik
	dari sistem paralel besar meskipun mereka memiliki sedikit atau
	tidak ada pengetahuan tentang konsep pemrograman paralel.
\end{description}

\begin{figure}
\centering
\resizebox{2.5in}{!}{\includegraphics{intro/PPGrelation}}
\caption{Software Layers and Performance, Productivity, and Generality}
\label{fig:intro:Software Layers and Performance; Productivity; and Generality}
\end{figure}

Nirvana dari lingkungan pemrograman paralel, yang menawarkan kinerja
dunia, produktivitas, dan umum, tidak ada.
Sampai nirvana itu muncul, akan diperlukan pertukaran teknis antara
kinerja, produktivitas, dan umum.
Salah satu pertukaran teknis ini ditampilkan oleh ``iron triangle'' berwarna
hijau yang ditunjukkan di \cref{fig:intro:Software Layers and Performance;
Productivity; and Generality}, yang menunjukkan bahwa produktivitas menjadi


The nirvana of parallel programming environments, one that offers
world-class performance, productivity, and generality, simply does
not yet exist.
Until such a nirvana appears, it will be necessary to make engineering
tradeoffs among performance, productivity, and generality.
One such tradeoff is depicted by the green ``iron triangle''\footnote{
	Kudos to Michael Wong for coining ``iron triangle.''}
shown in
\cref{fig:intro:Software Layers and Performance; Productivity; and Generality},
which shows how productivity becomes increasingly important at the upper layers
of the system stack,
while performance and generality become increasingly important at the
lower layers of the system stack.
The huge development costs incurred at the lower layers
must be spread over equally huge numbers of users
(hence the importance of generality), and
performance lost in lower layers cannot easily be
recovered further up the stack.
In the upper layers of the stack, there might be very few users for a given
specific application, in which case productivity concerns are paramount.
This explains the tendency towards ``bloatware'' further up the stack:
Extra hardware is often cheaper than extra developers.
This book is intended for developers working near the bottom
of the stack, where performance and generality are of greatest concern.

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{intro/Generality}}
\caption{Tradeoff Between Productivity and Generality}
\label{fig:intro:Tradeoff Between Productivity and Generality}
\end{figure}

It is important to note that a tradeoff between productivity and
generality has existed for centuries in many fields.
For but one example, a nailgun is more productive than a hammer for
driving nails, but in contrast to the nailgun, a hammer can be used for
many things besides driving nails.
It should therefore be no surprise to see similar tradeoffs
appear in the field of parallel computing.
This tradeoff is shown schematically in
\cref{fig:intro:Tradeoff Between Productivity and Generality}.
Here, users~1, 2, 3, and~4 have specific jobs that they need the computer
to help them with.
The most productive possible language or environment for a given user is one
that simply does that user's job, without requiring any programming,
configuration, or other setup.

\QuickQuiz{
	This is a ridiculously unachievable ideal!
	Why not focus on something that is achievable in practice?
}\QuickQuizAnswer{
	This is eminently achievable.
	The cellphone is a computer that can be used to make phone
	calls and to send and receive text messages with little or
	no programming or configuration on the part of the end user.

	This might seem to be a trivial example at first glance,
	but if you consider it carefully you will see that it is
	both simple and profound.
	When we are willing to sacrifice generality, we can achieve
	truly astounding increases in productivity.
	Those who indulge in excessive generality will therefore fail to set
	the productivity bar high enough to succeed near the top of the
	software stack.
	This fact of life even has its own acronym:
	YAGNI, or ``You Ain't Gonna Need It.''
}\QuickQuizEnd

Unfortunately, a system that does the job required by user~1 is
unlikely to do user~2's job.
In other words, the most productive languages and environments are
domain-specific, and thus by definition lacking generality.

Another option is to tailor a given programming language or environment
to the hardware system (for example, low-level languages such as
assembly, C, C++, or Java) or to some abstraction (for example,
Haskell, Prolog, or Snobol), as is shown by the circular region near
the center of
\cref{fig:intro:Tradeoff Between Productivity and Generality}.
These languages can be considered to be general in the sense that they
are equally ill-suited to the jobs required by users~1, 2, 3, and~4.
In other words, their generality comes at the expense of
decreased productivity when compared to domain-specific languages
and environments.
Worse yet, a language that is tailored to a given abstraction
is likely to suffer from performance and scalability problems
unless and until it can be efficiently mapped to real hardware.

Is there no escape from iron triangle's three conflicting goals of
performance, productivity, and generality?

It turns out that there often is an escape, for example,
using the alternatives to parallel programming discussed in the next section.
After all, parallel programming can be a great deal of fun, but
it is not always the best tool for the job.

\section{Alternatives to Parallel Programming}
\label{sec:intro:Alternatives to Parallel Programming}
%
\epigraph{Experiment is folly when experience shows the way.}
	 {Roger M. Babson}

In order to properly consider alternatives to parallel programming,
you must first decide on what exactly you expect the parallelism
to do for you.
As seen in \cref{sec:intro:Parallel Programming Goals},
the primary goals of parallel programming are performance, productivity,
and generality.
Because this book is intended for developers working on
performance-critical code near the bottom of the software stack,
the remainder of this section focuses primarily on performance improvement.

It is important to keep in mind that parallelism is but one way to
improve performance.
Other well-known approaches include the following, in roughly increasing
order of difficulty:

\begin{enumerate}
\item	Run multiple instances of a sequential application.
\item	Make the application use existing parallel software.
\item	Optimize the serial application.
\end{enumerate}

These approaches are covered in the following sections.

\subsection{Multiple Instances of a Sequential Application}
\label{sec:intro:Multiple Instances of a Sequential Application}

Running multiple instances of a sequential application can allow you
to do parallel programming without actually doing parallel programming.
There are a large number of ways to approach this, depending on the
structure of the application.

If your program is analyzing a large number of different scenarios,
or is analyzing a large number of independent data sets, one easy
and effective approach is to create a single sequential program that
carries out a single analysis, then use any of a number of scripting
environments (for example the \co{bash} shell) to run a number of
instances of that sequential program in parallel.
In some cases, this approach can be easily extended to a cluster of
machines.

This approach may seem like cheating, and in fact some denigrate such
programs as ``\IX{embarrassingly parallel}''.
And in fact, this approach does have some potential disadvantages,
including increased memory consumption, waste of CPU cycles recomputing
common intermediate results, and increased copying of data.
However, it is often  extremely productive, garnering extreme performance
gains with little or no added effort.

\subsection{Use Existing Parallel Software}
\label{sec:intro:Use Existing Parallel Software}

There is no longer any shortage of parallel software environments that
can present a single-threaded programming environment,
including relational
databases~\cite{Date82},
web-application servers, and map-reduce environments.
For example, a common design provides a separate process for each
user, each of which generates SQL from user queries.
This per-user SQL is run against a common relational database, which
automatically runs the users' queries concurrently.
The per-user programs are responsible only for the user interface,
with the relational database taking full responsibility for the
difficult issues surrounding parallelism and persistence.

In addition, there are a growing number of parallel library functions,
particularly for numeric computation.
Even better, some libraries take advantage of special\-/purpose
hardware such as vector units and general\-/purpose graphical processing
units (GPGPUs).

Taking this approach often sacrifices some performance, at least when
compared to carefully hand-coding a fully parallel application.
However, such sacrifice is often well repaid by a huge reduction in
development effort.

\QuickQuiz{
	Wait a minute!
	Doesn't this approach simply shift the development effort from
	you to whoever wrote the existing parallel software you are using?
}\QuickQuizAnswer{
	Exactly!
	And that is the whole point of using existing software.
	One team's work can be used by many other teams, resulting in a
	large decrease in overall effort compared to all teams
	needlessly reinventing the wheel.
}\QuickQuizEnd

\subsection{Performance Optimization}
\label{sec:intro:Performance Optimization}

Up through the early 2000s, CPU clock frequencies doubled every 18 months.
It was therefore usually more important to create new functionality than to
carefully optimize performance.
Now that \IXr{Moore's Law} is ``only'' increasing transistor density instead
of increasing both transistor density and per-transistor performance,
it might be a good time to rethink the importance of performance
optimization.
After all, new hardware generations no longer bring significant
single-threaded performance improvements.
Furthermore, many performance optimizations can also conserve energy.

From this viewpoint, parallel programming is but another performance
optimization, albeit one that is becoming much more attractive
as parallel systems become cheaper and more readily available.
However, it is wise to keep in mind that the speedup available from
parallelism is limited to roughly the number of CPUs
(but see \cref{sec:SMPdesign:Beyond Partitioning}
for an interesting exception).
In contrast, the speedup available from traditional single-threaded
software optimizations can be much larger.
For example, replacing a long linked list with a hash table
or a search tree can improve performance by many orders of magnitude.
This highly optimized single-threaded program might run much
faster than its unoptimized parallel counterpart, making parallelization
unnecessary.
Of course, a highly optimized parallel program would be even better,
aside from the added development effort required.

Furthermore, different programs might have different performance
bottlenecks.
For example, if your program spends most of its time
waiting on data from your disk drive,
using multiple CPUs will probably just increase the time wasted waiting
for the disks.
In fact, if the program was reading from a single large file laid out
sequentially on a rotating disk, parallelizing your program might
well make it a lot slower due to the added seek overhead.
You should instead optimize the data layout so that
the file can be smaller (thus faster to read), split the file into chunks
which can be accessed in parallel from different drives,
cache frequently accessed data in main memory,
or, if possible,
reduce the amount of data that must be read.

\QuickQuiz{
	What other bottlenecks might prevent additional CPUs from
	providing additional performance?
}\QuickQuizAnswer{
	There are any number of potential bottlenecks:
	\begin{enumerate}
	\item	Main memory.
		If a single thread consumes all available
		memory, additional threads will simply page themselves
		silly.
	\item	Cache.
		If a single thread's cache footprint completely
		fills any shared CPU cache(s), then adding more threads
		will simply thrash those affected caches, as will be
		seen in \cref{chp:Data Structures}.
	\item	Memory bandwidth.
		If a single thread consumes all available
		memory bandwidth, additional threads will simply
		result in additional queuing on the system interconnect.
	\item	I/O bandwidth.
		If a single thread is I/O bound,
		adding more threads will simply result in them all
		waiting in line for the affected I/O resource.
	\end{enumerate}

	Specific hardware systems might have any number of additional
	bottlenecks.
	The fact is that every resource which is shared between
	multiple CPUs or threads is a potential bottleneck.
}\QuickQuizEnd

Parallelism can be a powerful optimization technique, but
it is not the only such technique, nor is it appropriate for all
situations.
Of course, the easier it is to parallelize your program, the
more attractive parallelization becomes as an optimization.
Parallelization has a reputation of being quite difficult,
which leads to the question ``exactly what makes parallel
programming so difficult?''

\section{What Makes Parallel Programming Hard?}
\label{sec:intro:What Makes Parallel Programming Hard?}
%
\epigraph{Real difficulties can be overcome; it is only the imaginary
	  ones that are unconquerable.}{Theodore N.~Vail}

\OriginallyPublished{Section}{sec:intro:What Makes Parallel Programming Hard?}{What Makes Parallel Programming Hard?}{a Portland State University Technical Report}{PaulEMcKenney2009ProgrammingHard}

It is important to note that the difficulty of parallel programming
is as much a human-factors issue as it is a set of technical properties of the
parallel programming problem.
We do need human beings to be able to tell parallel
systems what to do, otherwise known as programming.
But parallel programming involves two-way communication, with
a program's performance and scalability being the communication from
the machine to the human.
In short, the human writes a program telling the computer what to do,
and the computer critiques this program via the resulting performance and
scalability.
Therefore, appeals to abstractions or to mathematical analyses will
often be of severely limited utility.

In the Industrial Revolution, the interface between human and machine
was evaluated by human-factor studies, then called time-and-motion
studies.
Although there have been a few human-factor studies examining parallel
programming~\cite{RyanEccles2005HPCSNovice,RyanEccles2006HPCSNoviceNeeds,
LorinHochstein2005SC,DuaneSzafron1994PEMPDS}, these studies have
been extremely narrowly focused, and hence unable to demonstrate any
general results.
Furthermore, given that the normal range of programmer productivity
spans more than an order of magnitude, it is unrealistic to expect
an affordable study to be capable of detecting (say) a 10\,\% difference
in productivity.
Although the multiple-order-of-magnitude differences that such studies
\emph{can} reliably detect are extremely valuable, the most impressive
improvements tend to be based on a long series of 10\,\% improvements.

We must therefore take a different approach.

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{intro/FourTaskCategories}}
\caption{Categories of Tasks Required of Parallel Programmers}
\label{fig:intro:Categories of Tasks Required of Parallel Programmers}
\end{figure}

One such approach is to carefully consider the tasks that parallel
programmers must undertake that are not required of sequential programmers.
We can then evaluate how well a given programming language or environment
assists the developer with these tasks.
These tasks fall into the four categories shown in
\cref{fig:intro:Categories of Tasks Required of Parallel Programmers},
each of which is covered in the following sections.

\subsection{Work Partitioning}
\label{sec:intro:Work Partitioning}

Work partitioning is absolutely required for parallel execution:
If there is but one ``glob'' of work, then it can be executed by at
most one CPU at a time, which is by definition sequential execution.
However, partitioning the work requires great care.
For example, uneven partitioning can result in sequential execution
once the small partitions have completed~\cite{GeneAmdahl1967AmdahlsLaw}.
In less extreme cases, load balancing can be used to fully utilize
available hardware and restore performance and scalability.

Although partitioning can greatly improve performance and scalability,
it can also increase complexity.
For example, partitioning can complicate handling of global
errors and events:
A parallel program may need to carry out non-trivial synchronization
in order to safely process such global events.
More generally, each partition requires some sort of communication:
After all, if
a given thread did not communicate at all, it would have no effect and
would thus not need to be executed.
However, because communication incurs overhead, careless partitioning choices
can result in severe performance degradation.

Furthermore, the number of concurrent threads must often be controlled,
as each such thread occupies common resources, for example,
space in CPU caches.
If too many threads are permitted to execute concurrently, the
CPU caches will overflow, resulting in high cache miss rate, which in
turn degrades performance.
Conversely, large numbers of threads are often required to
overlap computation and I/O so as to fully utilize I/O devices.

\QuickQuiz{
	Other than CPU cache capacity, what might require limiting the
	number of concurrent threads?
}\QuickQuizAnswer{
	There are any number of potential limits on the number of
	threads:
	\begin{enumerate}
	\item	Main memory.
		Each thread consumes some memory
		(for its stack if nothing else), so that excessive
		numbers of threads can exhaust memory, resulting
		in excessive paging or memory-allocation failures.
	\item	I/O bandwidth.
		If each thread initiates a given
		amount of mass-storage I/O or networking traffic,
		excessive numbers of threads can result in excessive
		I/O queuing delays, again degrading performance.
		Some networking protocols may be subject to timeouts
		or other failures if there are so many threads that
		networking events cannot be responded to in a timely
		fashion.
	\item	Synchronization overhead.
		For many synchronization protocols, excessive numbers
		of threads can result in excessive spinning, blocking,
		or rollbacks, thus degrading performance.
	\end{enumerate}

	Specific applications and platforms may have any number of additional
	limiting factors.
}\QuickQuizEnd

Finally, permitting threads to execute concurrently greatly increases
the program's state space, which can make the program difficult to
understand and debug, degrading productivity.
All else being equal, smaller state spaces having more regular structure
are more easily understood, but this is a human-factors statement as much
as it is a technical or mathematical statement.
Good parallel designs might have extremely large state spaces, but
nevertheless be easy to understand due to their regular structure,
while poor designs can be impenetrable despite having a comparatively
small state space.
The best designs exploit embarrassing parallelism, or transform the
problem to one having an embarrassingly parallel solution.
In either case, ``embarrassingly parallel'' is in fact
an embarrassment of riches.
The current state of the art enumerates good designs; more work is
required to make more general judgments on
state-space size and structure.

\subsection{Parallel Access Control}
\label{sec:Parallel Access Control}

Given a single-threaded sequential program, that single
thread has full access to all of the program's resources.
These resources are most often in-memory data structures, but can be CPUs,
memory (including caches), I/O devices, computational accelerators, files,
and much else besides.

The first parallel-access-control issue is whether the form of access to
a given resource depends on that resource's location.
For example, in many message-passing environments, local-variable
access is via expressions and assignments,
while remote-variable access uses an entirely different
syntax, usually involving messaging.
The POSIX Threads environment~\cite{OpenGroup1997pthreads},
Structured Query Language (SQL)~\cite{DIS9075SQL92}, and
partitioned global address-space (PGAS) environments
such as Universal Parallel C (UPC)~\cite{ElGhazawi2003UPC,UPCConsortium2013}
offer implicit access,
while Message Passing Interface (MPI)~\cite{MPIForum2008} offers
explicit access because access to remote data requires explicit
messaging.

The other parallel-access-control issue is how threads coordinate
access to the resources.
This coordination is carried out by
the very large number of synchronization mechanisms
provided by various parallel languages and environments,
including message passing, locking, transactions,
reference counting, explicit timing, shared atomic variables, and data
ownership.
Many traditional parallel-programming concerns such as \IX{deadlock},
\IX{livelock}, and transaction rollback stem from this coordination.
This framework can be elaborated to include comparisons
of these synchronization mechanisms, for example locking vs.\@ transactional
memory~\cite{McKenney2007PLOSTM}, but such elaboration is beyond the
scope of this section.
(See
\cref{sec:future:Transactional Memory,%
sec:future:Hardware Transactional Memory}
for more information on transactional memory.)

\QuickQuiz{
	Just what is ``explicit timing''???
}\QuickQuizAnswer{
	Where each thread is given access to some set of resources during
	an agreed-to slot of time.
	For example, a parallel program with eight threads might be
	organized into eight-millisecond time intervals, so that the
	first thread is given access during the first millisecond of
	each interval, the second thread during the second millisecond,
	and so on.
	This approach clearly requires carefully synchronized clocks
	and careful control of execution times, and therefore should
	be used with considerable caution.

	In fact, outside of hard realtime environments, you almost
	certainly want to use something else instead.
	Explicit timing is nevertheless worth a mention, as it is
	always there when you need it.
}\QuickQuizEnd

\subsection{Resource Partitioning and Replication}
\label{sec:Resource Partitioning and Replication}

The most effective parallel algorithms and systems exploit resource
parallelism, so much so that it is
usually wise to begin parallelization by partitioning your write-intensive
resources and replicating frequently accessed read-mostly resources.
The resource in question is most frequently data, which might be
partitioned over computer systems, mass-storage devices, \IXplr{NUMA node},
CPU cores (or dies or hardware threads), pages, cache lines, instances
of synchronization primitives, or critical sections of code.
For example, partitioning over locking primitives is termed
``\IXh{data}{locking}''~\cite{Beck85}.

Resource partitioning is frequently application dependent.
For example, numerical applications frequently partition matrices
by row, column, or sub-matrix, while commercial applications frequently
partition write-intensive data structures and replicate
read-mostly data structures.
Thus, a commercial application might assign the data for a
given customer to a given few computers out of a large cluster.
An application might statically partition data, or dynamically
change the partitioning over time.

Resource partitioning is extremely effective, but
it can be quite challenging for complex multilinked data
structures.

\subsection{Interacting With Hardware}
\label{sec:Interacting With Hardware}

Hardware interaction is normally the domain of the operating system,
the compiler, libraries, or other software-environment infrastructure.
However, developers working with novel hardware features and components
will often need to work directly with such hardware.
In addition, direct access to the hardware can be required when squeezing
the last drop of performance out of a given system.
In this case, the developer may need to tailor or configure the application
to the cache geometry, system topology, or interconnect protocol of the
target hardware.

In some cases, hardware may be considered to be a resource which
is subject to partitioning or access control, as described in
the previous sections.

\subsection{Composite Capabilities}
\label{sec:Composite Capabilities}

\begin{figure}
\centering
\resizebox{3in}{!}{\includegraphics{intro/FourTaskOrder}}
\caption{Ordering of Parallel-Programming Tasks}
\label{fig:intro:Ordering of Parallel-Programming Tasks}
\end{figure}

Although these four capabilities are fundamental,
good engineering practice uses composites of
these capabilities.
For example, the data-parallel approach first
partitions the data so as to minimize the need for
inter-partition communication, partitions the code accordingly,
and finally maps data partitions and threads so as to maximize
throughput while minimizing inter-thread communication,
as shown in
\cref{fig:intro:Ordering of Parallel-Programming Tasks}.
The developer can then
consider each partition separately, greatly reducing the size
of the relevant state space, in turn increasing productivity.
Even though some problems are non-partitionable,
clever transformations into forms permitting partitioning can
sometimes greatly enhance
both performance and scalability~\cite{PanagiotisMetaxas1999PDCS}.

\subsection{How Do Languages and Environments Assist With These Tasks?}
\label{sec:intro:How Do Languages and Environments Assist With These Tasks?}

Although many environments require the developer to deal manually
with these tasks, there are long-standing environments that bring
significant automation to bear.
The poster child for these environments is SQL, many implementations
of which automatically parallelize single large queries and also
automate concurrent execution of independent queries and updates.

These four categories of tasks must be carried out in all parallel
programs, but that of course does not necessarily mean that the developer
must manually carry out these tasks.
We can expect to see ever-increasing automation of these four tasks
as parallel systems continue to become cheaper and more readily available.

\QuickQuiz{
	Are there any other obstacles to parallel programming?
}\QuickQuizAnswer{
	There are a great many other potential obstacles to parallel
	programming.
	Here are a few of them:
	\begin{enumerate}
	\item	The only known algorithms for a given project might
		be inherently sequential in nature.
		In this case, either avoid parallel programming
		(there being no law saying that your project \emph{has}
		to run in parallel) or invent a new parallel algorithm.
	\item	The project allows binary-only plugins that share the same
		address space, such that no one developer has access to
		all of the source code for the project.
		Because many parallel bugs, including deadlocks, are
		global in nature, such binary-only plugins pose a severe
		challenge to current software development methodologies.
		This might well change, but for the time being, all
		developers of parallel code sharing a given address space
		need to be able to see \emph{all} of the code running in
		that address space.
	\item	The project contains heavily used APIs that were designed
		without regard to
		parallelism~\cite{HagitAttiya2011LawsOfOrder,Clements:2013:SCR:2517349.2522712}.
		Some of the more ornate features of the System V
		message-queue API form a case in point.
		Of course, if your project has been around for a few
		decades, and its developers did not have access to
		parallel hardware, it undoubtedly has at least
		its share of such APIs.
	\item	The project was implemented without regard to parallelism.
		Given that there are a great many techniques that work
		extremely well in a sequential environment, but that
		fail miserably in parallel environments, if your project
		ran only on sequential hardware for most of its lifetime,
		then your project undoubtably has at least its share of
		parallel-unfriendly code.
	\item	The project was implemented without regard to good
		software-development practice.
		The cruel truth is that shared-memory parallel
		environments are often much less forgiving of sloppy
		development practices than are sequential environments.
		You may be well-served to clean up the existing design
		and code prior to attempting parallelization.
	\item	The people who originally did the development on your
		project have since moved on, and the people remaining,
		while well able to maintain it or add small features,
		are unable to make ``big animal'' changes.
		In this case, unless you can work out a very simple
		way to parallelize your project, you will probably
		be best off leaving it sequential.
		That said, there are a number of simple approaches that
		you might use
		to parallelize your project, including running multiple
		instances of it, using a parallel implementation of
		some heavily used library function, or making use of
		some other parallel project, such as a database.
	\end{enumerate}

	One can argue that many of these obstacles are non-technical
	in nature, but that does not make them any less real.
	In short, parallelization of a large body of code
	can be a large and complex effort.
	As with any large and complex effort, it makes sense to
	do your homework beforehand.
}\QuickQuizEnd

\section{Discussion}
\label{sec:intro:Discussion}
%
\epigraph{Until you try, you don't know what you can't do.}
	 {Henry James}

This section has given an overview of the difficulties with, goals of,
and alternatives to parallel programming.
This overview was followed by a discussion of
what can make parallel programming hard, along with a high-level
approach for dealing with parallel programming's difficulties.
Those who still insist that parallel programming is impossibly difficult
should review some of the older guides to parallel
programmming~\cite{SQNTParallel,AndrewDBirrell1989Threads,Beck85,Inman85}.
The following quote from Andrew Birrell's
monograph~\cite{AndrewDBirrell1989Threads} is especially telling:

\begin{quote}
	Writing concurrent programs has a reputation for being exotic
	and difficult.
	I~believe it is neither.
	You need a system that provides you with good primitives
	and suitable libraries,
	you need a basic caution and carefulness, you need an armory of
	useful techniques, and you need to know of the common pitfalls.
	I~hope that this paper has helped you towards sharing my belief.
\end{quote}

The authors of these older guides were well up to the parallel programming
challenge back in the 1980s.
As such, there are simply no excuses for refusing to step up to the
parallel-programming challenge here in the 21\textsuperscript{st} century!

We are now ready to proceed to the next chapter, which dives into the
relevant properties of the parallel hardware underlying our parallel
software.

\QuickQuizAnswersChp{qqzintro}
